
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 

\documentclass[twocolumn]{article}

% Packages
% \usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=10mm]{geometry}
\usepackage{graphicx}



% Housekeeping
\pagestyle{empty}

\begin{document}
\begin{itemize}
\item Foreword: I love this place indeed and therefore I shall work as
  hard as possible (like every hardworking Chinese) to stay, and make
  contributions. 
\item Collaborative filtering: Generally a form of supervised learning
  but with very special conditions.
\item Classes of CF algorithms:
  \begin{itemize}
  \item Memory based: Final prediction of rating typically obtained
    via a weighted sum of neighbors.
  \item Model based: Based on constructing a model which describes
    important properties of the data.
  \end{itemize}
\item Association rule mining: Unable to generate recommendations for
  first time users, therefore it's user-based.
\item Matrix factorization for CF\@: $R~=~P\ast Q$. Steps are as
  follows:
  \begin{enumerate}
  \item Take, as input, matrix R, with elements $r_{ij}$.
  \item Create component matrices P and Q, by initializing randomly.
  \item Loop over all element of R which has been rated.
  \item Iterate until convergence.
  \end{enumerate}
% \item Is life hard? When compared with many other people working in
%   the industry, it is not hard at all. So please do not
%   complain. Learn those things, and make contributions.
\item Regularized Matrix Factorization: By adjusting the prior
  distribution for the parameters, optimize the posterior distribution
  instead. (Reflected in years' exams. Try them tonight.)
% \item Life is not hard. Indeed.
\item EM Algorithm: 
  \begin{itemize}
  \item Define full/complete data as $D={x,z}$. $x$ is the observed
    data, $z$ is the hidden or missing data.
  \item $\theta$, the parameters of the model, also referred to as the
    complete data likelihood.
  \end{itemize}
  In practice, we would like to optimize the log likelihood of the
  parameters based on the observed data. We can optimize this directly
  but in general it is very difficult to do so.
  $$\theta_{ML}~=~argmax_{\theta}\log p(x;\theta)$$
  $$\theta_{ML}~=~argmax_{\theta}\log\sum_{z}p(x,z;\theta)$$
\item $$L(\theta)~=~\log\sum_{z}p(x,z;\theta)$$
  $$L(\theta)~=~\log\sum_{z}Q(z)\frac{p(x,z;\theta)}{Q(z)}$$
  $$L(\theta)\geq\sum_{z}Q(z)\log\frac{p(x,z;\theta)}{Q(z)}$$
\item How do we make the bound tight? \textbf{we can make this tight if
    the expression in the log is a constant value}.
\item $$\frac{p(x,z;\hat{\theta})}{Q(z)}=\frac{p(z|x;\hat{\theta})p(x;\hat\theta)}{Q(z)}$$
  Therefore we set $Q(z)~=~p(z|x;\hat{\theta})$ where $\hat{\theta}$
  is the current best guess of $\theta$.
\item
  \begin{itemize}
  \item \textbf{Expectation:}  Finding the distribution of z, given
    the data, and the current best guess of $\theta$.
  \item Calculating the expectation over the incomplete data
    likelihood over the current best guess of $\theta$.
    \begin{equation}
      \label{eq:1}
      E_{\hat{\theta}}[p(x|\theta)]~=~\sum_{z}p(z|x;\hat{\theta})\log\frac{p(x,z;\theta)}{p(z|x;\hat{\theta})}
    \end{equation}
  \item \textbf{Maximization:} Optimize the equation with respect to $\theta$.
  \end{itemize}
\item Some disadvantages for EM algorithm: it is still a
  maximum-likelihood approach, not Bayesian.
\item In EM algorithm, what we are trying to maximize is the complete
  data log likelihood.
% \item Some things shall not left unfinished. Or you will regret.
\item Perceptron algorithm: it considers each training point in turn,
  adjusting the parameters to correct any
  mistakes. $\theta_{n+1}~=~\theta_{n}+y_{i}x_{i}$. It will converge
  if the training points are \emph{linearly separable through
    origin}. Otherwise it won't converge.
\item 
\end{itemize}
\end{document}